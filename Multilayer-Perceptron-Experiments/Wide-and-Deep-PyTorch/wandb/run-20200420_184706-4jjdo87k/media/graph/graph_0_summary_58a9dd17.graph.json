{"format": "torch", "nodes": [{"name": "embedding", "id": 4994022536, "class_name": "FeaturesEmbedding(\n  (embedding): Embedding(9992, 16)\n)", "parameters": [["embedding.weight", [9992, 16]]], "output_shape": [[512, 2, 16]], "num_parameters": [159872]}, {"name": "linear", "id": 4994022312, "class_name": "FeaturesLinear(\n  (fc): Embedding(9992, 1)\n)", "parameters": [["bias", [1]], ["fc.weight", [9992, 1]]], "output_shape": [[512, 1]], "num_parameters": [1, 9992]}, {"name": "mlp", "id": 5024861880, "class_name": "MultiLayerPerceptron(\n  (mlp): Sequential(\n    (0): Linear(in_features=32, out_features=16, bias=True)\n    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=16, out_features=16, bias=True)\n    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=16, out_features=1, bias=True)\n  )\n)", "parameters": [["mlp.0.weight", [16, 32]], ["mlp.0.bias", [16]], ["mlp.1.weight", [16]], ["mlp.1.bias", [16]], ["mlp.4.weight", [16, 16]], ["mlp.4.bias", [16]], ["mlp.5.weight", [16]], ["mlp.5.bias", [16]], ["mlp.8.weight", [1, 16]], ["mlp.8.bias", [1]]], "output_shape": [[512, 1]], "num_parameters": [512, 16, 16, 16, 256, 16, 16, 16, 16, 1]}], "edges": []}