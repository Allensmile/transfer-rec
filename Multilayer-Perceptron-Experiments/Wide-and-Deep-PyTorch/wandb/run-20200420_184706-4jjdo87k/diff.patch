diff --git a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/README.md b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/README.md
index 0e626db..a3d1a5b 100644
--- a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/README.md
+++ b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/README.md
@@ -31,9 +31,9 @@ Here are the model hyper-parameters chosen:
 - Number of Hidden Layers used in the Deep Component = 16
 - Activation Function = Sigmoid
 - Learning Rate = 0.001
-- Batch Size = 2048
+- Batch Size = 512
 - Weight Decay = 0.000001
 - Optimizer Method = Adam
-- Dropout Rate = 0.2
+- Dropout Rate = 0.5
 
-After being trained for 100 epochs, the model achieves **validation AUC = 0.791** and **test AUC = 0.788**.
\ No newline at end of file
+After being trained for 100 epochs, the model achieves **validation AUC = 0.7982** and **test AUC = 0.7963** with **runtime = 1h 39m 57s**
\ No newline at end of file
diff --git a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/__pycache__/Wide_Deep.cpython-36.pyc b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/__pycache__/Wide_Deep.cpython-36.pyc
index 73f39bf..57f0abf 100644
Binary files a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/__pycache__/Wide_Deep.cpython-36.pyc and b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/__pycache__/Wide_Deep.cpython-36.pyc differ
diff --git a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/chkpt/wd.pt b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/chkpt/wd.pt
deleted file mode 100644
index 0cabf6b..0000000
Binary files a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/chkpt/wd.pt and /dev/null differ
diff --git a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/main.py b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/main.py
index ab7c4ec..afd1399 100644
--- a/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/main.py
+++ b/Multilayer-Perceptron-Experiments/Wide-and-Deep-PyTorch/main.py
@@ -6,6 +6,9 @@ from torch.utils.data import DataLoader
 from data import MovieLens1MDataset
 from Wide_Deep import WideAndDeepModel
 
+# Initialize Weights and Biases
+import wandb
+wandb.init(project="multi_layer_perceptron_collaborative_filtering")
 
 def get_dataset(name, path):
     """
@@ -30,7 +33,7 @@ def get_model(name, dataset):
     field_dims = dataset.field_dims
     if name == 'wd':
         # Hyperparameters are empirically determined, not optimized
-        return WideAndDeepModel(field_dims, embed_dim=16, mlp_dims=(16, 16), dropout=0.2)
+        return WideAndDeepModel(field_dims, embed_dim=16, mlp_dims=(16, 16), dropout=0.5)
     else:
         raise ValueError('unknown model name: ' + name)
 
@@ -62,6 +65,8 @@ def train(model, optimizer, data_loader, criterion, device, log_interval=1000):
             print('    - loss:', total_loss / log_interval)
             total_loss = 0
 
+        wandb.log({"Total Loss": total_loss})
+
 
 def test(model, data_loader, device):
     """
@@ -123,21 +128,29 @@ def main(dataset_name, dataset_path, model_name, epoch, learning_rate,
     # Use Adam optimizer
     optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)
 
+    # Log metrics with Weights and Biases
+    wandb.watch(model, log="all")
+
     # Loop through pre-defined number of epochs
     for epoch_i in range(epoch):
         # Perform training on the train set
         train(model, optimizer, train_data_loader, criterion, device)
         # Perform evaluation on the validation set
-        auc = test(model, valid_data_loader, device)
+        valid_auc = test(model, valid_data_loader, device)
         # Log the epochs and AUC on the validation set
-        print('epoch:', epoch_i, 'validation: auc:', auc)
+        print('epoch:', epoch_i, 'validation: auc:', valid_auc)
+        wandb.log({"Validation AUC": valid_auc})
 
     # Perform evaluation on the test set
-    auc = test(model, test_data_loader, device)
+    test_auc = test(model, test_data_loader, device)
     # Log the final AUC on the test set
-    print('test auc:', auc)
+    print('test auc:', test_auc)
+    wandb.log({"Test AUC": test_auc})
+
     # Save the model checkpoint
-    torch.save(model, f'{save_dir}/{model_name}.pt')
+    torch.save(model.state_dict(), f'{save_dir}/{model_name}.pt')
+    # Save model to Weights and Biases portal
+    wandb.save(f'{model_name}.pt')
 
 
 if __name__ == '__main__':
@@ -149,7 +162,7 @@ if __name__ == '__main__':
     parser.add_argument('--model_name', default='wd')
     parser.add_argument('--epoch', type=int, default=100)
     parser.add_argument('--learning_rate', type=float, default=0.001)
-    parser.add_argument('--batch_size', type=int, default=2048)
+    parser.add_argument('--batch_size', type=int, default=512)
     parser.add_argument('--weight_decay', type=float, default=1e-6)
     parser.add_argument('--device', default='cpu')
     parser.add_argument('--save_dir', default='chkpt')
